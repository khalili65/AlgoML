# Regression: Complete Interview Guide

> Predicting continuous values by modeling the relationship between dependent and independent variables.

---

## ðŸ“Š Overview

**Regression** is a supervised learning technique used to predict a **continuous target variable** based on one or more predictor variables.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGRESSION TYPES                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Simple Linear        â†’  y = Î²â‚€ + Î²â‚x                       â”‚
â”‚  Multiple Linear      â†’  y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™  â”‚
â”‚  Polynomial           â†’  y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + ... + Î²â‚™xâ¿   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“ˆ Simple Linear Regression

### The Model

```
y = Î²â‚€ + Î²â‚x + Îµ
```

| Symbol | Meaning |
|--------|---------|
| y | Dependent variable (target) |
| x | Independent variable (predictor) |
| Î²â‚€ | Y-intercept (value of y when x = 0) |
| Î²â‚ | Slope (change in y for unit change in x) |
| Îµ | Error term (residual) |

### Goal

Find the **best-fitting line** that minimizes the **sum of squared differences** between actual and predicted values.

```
Minimize: Î£(yáµ¢ - Å·áµ¢)Â² = Î£(yáµ¢ - (Î²â‚€ + Î²â‚xáµ¢))Â²
```

### Visual Intuition

```
  y
  â†‘
  â”‚       â€¢    
  â”‚    â€¢    â€¢   â†â”€â”€ Actual points
  â”‚  â€¢  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†â”€â”€ Best fit line (Å· = Î²â‚€ + Î²â‚x)
  â”‚â€¢     â€¢
  â”‚   â€¢
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
```

---

## ðŸ“ Multiple Linear Regression

### The Model

When you have **multiple predictor variables**:

```
y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™ + Îµ
```

### Example: House Price Prediction

```
Price = Î²â‚€ + Î²â‚(Size) + Î²â‚‚(Bedrooms) + Î²â‚ƒ(Age) + Îµ
```

Each coefficient Î²áµ¢ represents the **change in y** for a **one-unit change in xáµ¢**, holding all other variables constant.

### Matrix Form

```
y = XÎ² + Îµ

Where:
  y = [n Ã— 1] target vector
  X = [n Ã— (p+1)] design matrix (with intercept column of 1s)
  Î² = [(p+1) Ã— 1] coefficient vector
  Îµ = [n Ã— 1] error vector
```

### Solving for Î² (Ordinary Least Squares)

```
Î² = (Xáµ€X)â»Â¹Xáµ€y
```

---

## ðŸŽ¯ R-Squared (RÂ²): Coefficient of Determination

### Definition

> **RÂ²** measures the **proportion of variance** in the target variable that is **explained by the predictor variables**.

```
         Unexplained Variance       SSR        Î£(yáµ¢ - Å·áµ¢)Â²
RÂ² = 1 - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 1 - â”€â”€â”€â”€ = 1 - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
           Total Variance          SST        Î£(yáµ¢ - È³)Â²
```

### Components

| Term | Name | Formula | Meaning |
|------|------|---------|---------|
| **SST** | Total Sum of Squares | Î£(yáµ¢ - È³)Â² | Total variance in y |
| **SSR** | Residual Sum of Squares | Î£(yáµ¢ - Å·áµ¢)Â² | Unexplained variance |
| **SSE** | Explained Sum of Squares | Î£(Å·áµ¢ - È³)Â² | Explained variance |

**Relationship:** SST = SSE + SSR

### Interpretation

| RÂ² Value | Interpretation |
|----------|----------------|
| 0.0 | Model explains **none** of the variance |
| 0.5 | Model explains **50%** of the variance |
| 0.8 | Model explains **80%** of the variance |
| 1.0 | Model explains **all** of the variance (perfect fit) |

### Concrete Example: Student Study Hours

```
Students studying for a test:

Student  Hours(x)  Actual Score(y)  Predicted(Å·)
â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   A        1           70              71
   B        2           75              76
   C        3           80              81
   D        4           85              86
   E        5           90              91

Mean actual score (È³) = 80

Total Variance (SST):
  = (70-80)Â² + (75-80)Â² + (80-80)Â² + (85-80)Â² + (90-80)Â²
  = 100 + 25 + 0 + 25 + 100 = 250

Residual Variance (SSR):
  = (70-71)Â² + (75-76)Â² + (80-81)Â² + (85-86)Â² + (90-91)Â²
  = 1 + 1 + 1 + 1 + 1 = 5

RÂ² = 1 - (5/250) = 1 - 0.02 = 0.98

â†’ 98% of variance in scores is explained by hours studied!
```

### Visual Intuition

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNDERSTANDING RÂ²                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Total Variance (SST) = What we're trying to explain        â”‚
â”‚  â”œâ”€â”€ Explained by model (SSE) â”€â”€â”€â”€â†’ This is GOOD âœ“          â”‚
â”‚  â””â”€â”€ Unexplained/Residual (SSR) â”€â”€â†’ This is ERROR âœ—         â”‚
â”‚                                                             â”‚
â”‚  RÂ² = SSE/SST = Explained/Total = How much model explains   â”‚
â”‚                                                             â”‚
â”‚  High RÂ² â†’ Model captures most variance â†’ Good fit          â”‚
â”‚  Low RÂ²  â†’ Model misses most variance  â†’ Poor fit           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš–ï¸ Adjusted R-Squared

### The Problem with RÂ²

RÂ² **always increases** (or stays the same) when you add more predictors, even if they're useless!

```
Add irrelevant feature â†’ RÂ² goes up â†’ Misleading!
```

### The Solution: Adjusted RÂ²

**Adjusted RÂ²** penalizes for adding predictors that don't improve the model significantly.

```
                         (1 - RÂ²)(n - 1)
Adjusted RÂ² = 1 - â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        n - k - 1

Where:
  n = number of samples
  k = number of predictors
```

### Comparison

| Metric | Behavior | When to Use |
|--------|----------|-------------|
| **RÂ²** | Always increases with more features | Single model evaluation |
| **Adjusted RÂ²** | Can decrease if feature isn't useful | Comparing models with different # of features |

### Example

```
Model A: 3 features,  RÂ² = 0.85,  Adjusted RÂ² = 0.83
Model B: 10 features, RÂ² = 0.87,  Adjusted RÂ² = 0.79

â†’ Model A is better! Extra features in B don't help much.
```

---

## ðŸ“ Evaluation Metrics

### Mean Squared Error (MSE)

```
         1
MSE = â”€â”€â”€â”€â”€  Î£(yáµ¢ - Å·áµ¢)Â²
         n
```

- Average squared difference between actual and predicted
- **Lower is better**
- Units are squared (e.g., dollarsÂ² for price)

### Root Mean Squared Error (RMSE)

```
RMSE = âˆšMSE = âˆš[ (1/n) Î£(yáµ¢ - Å·áµ¢)Â² ]
```

- Same units as target variable
- **Lower is better**
- More interpretable than MSE

### Mean Absolute Error (MAE)

```
         1
MAE = â”€â”€â”€â”€â”€  Î£|yáµ¢ - Å·áµ¢|
         n
```

- Less sensitive to outliers than MSE/RMSE
- **Lower is better**

### Comparison

| Metric | Outlier Sensitivity | Interpretation |
|--------|---------------------|----------------|
| MSE | High (squares errors) | Average squared error |
| RMSE | High | Average error in original units |
| MAE | Low (absolute value) | Average absolute error |
| RÂ² | Moderate | Proportion of variance explained |

---

## ðŸ”¢ Handling Categorical Variables: Dummy Variables

### The Problem

Regression requires **numerical** inputs, but categorical variables are not numerical.

```
City: "New York", "Los Angeles", "Chicago"  â† Can't use directly!
```

### The Solution: One-Hot Encoding (Dummy Variables)

Create **binary columns** for each category:

```
Original:                    Dummy Encoded:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    City      â”‚            â”‚ City_NYC   â”‚ City_LA     â”‚ City_CHIâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ New York     â”‚     â†’      â”‚     1      â”‚      0      â”‚    0    â”‚
â”‚ Los Angeles  â”‚            â”‚     0      â”‚      1      â”‚    0    â”‚
â”‚ Chicago      â”‚            â”‚     0      â”‚      0      â”‚    1    â”‚
â”‚ New York     â”‚            â”‚     1      â”‚      0      â”‚    0    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Dummy Variable Trap

> **Important:** Use **n-1 dummy variables** for n categories to avoid multicollinearity!

```python
# WRONG: Creates multicollinearity
pd.get_dummies(df['City'])  # 3 columns for 3 cities

# CORRECT: Drop one category (reference category)
pd.get_dummies(df['City'], drop_first=True)  # 2 columns for 3 cities
```

### Why Drop One?

If you know City_NYC=0 and City_LA=0, then City_CHI **must** be 1.
The dropped category becomes the **reference/baseline** category.

---

## ðŸ“ˆ Polynomial Regression

### When Linear Isn't Enough

Sometimes the relationship is **non-linear**:

```
Linear: y = Î²â‚€ + Î²â‚x              (straight line)
Quadratic: y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ²    (parabola)
Cubic: y = Î²â‚€ + Î²â‚x + Î²â‚‚xÂ² + Î²â‚ƒxÂ³ (S-curve)
```

### Visual Example

```
  y                           y
  â†‘      Linear               â†‘     Polynomial (degree 2)
  â”‚    â”€â”€â”€â”€â”€â”€â”€â”€               â”‚         â•±â•²
  â”‚   â•±                       â”‚        â•±  â•²
  â”‚  â•±   â€¢ â€¢ â€¢                â”‚   â€¢  â€¢â•±    â•²â€¢  â€¢
  â”‚ â•±  â€¢     â€¢                â”‚  â€¢   â•±      â•²   â€¢
  â”‚â•± â€¢         â€¢              â”‚    â€¢          â€¢
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
     Poor fit!                     Better fit!
```

### Implementation

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Transform features to include polynomial terms
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# X = [[2], [3], [4]]
# X_poly = [[2, 4], [3, 9], [4, 16]]  â† includes x and xÂ²

# Fit linear regression on polynomial features
model = LinearRegression()
model.fit(X_poly, y)
```

### Choosing the Degree

| Degree | Risk |
|--------|------|
| Too low | Underfitting (high bias) |
| Too high | Overfitting (high variance) |

Use **cross-validation** to find optimal degree!

---

## ðŸ”¬ Assumptions of Linear Regression

### The 5 Key Assumptions

| # | Assumption | Meaning | How to Check |
|---|------------|---------|--------------|
| 1 | **Linearity** | Relationship is linear | Residual vs fitted plot |
| 2 | **Independence** | Observations are independent | Durbin-Watson test |
| 3 | **Homoscedasticity** | Constant variance of residuals | Residual plot (no funnel shape) |
| 4 | **Normality** | Residuals are normally distributed | Q-Q plot, Shapiro-Wilk test |
| 5 | **No Multicollinearity** | Predictors aren't highly correlated | VIF (Variance Inflation Factor) |

### Visual: Residual Diagnostics

```
GOOD Residual Plot          BAD Residual Plot (Heteroscedasticity)
       â€¢                           â€¢    â€¢  â€¢
    â€¢     â€¢                              â€¢    â€¢   â€¢
  â€¢    â€¢    â€¢                     â€¢         â€¢       â€¢
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0
    â€¢  â€¢   â€¢                    â€¢  â€¢
      â€¢                         â€¢
  Random scatter!               Funnel shape = problem!
```

---

## ðŸ› ï¸ Implementation in Python

### Using Scikit-Learn

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict
y_pred = model.predict(X_test)

# Evaluate
print(f"RÂ²: {r2_score(y_test, y_pred):.4f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}")
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
```

### Using Statsmodels (for p-values!)

```python
import statsmodels.api as sm

# Add constant for intercept
X_const = sm.add_constant(X)

# Fit OLS model
model = sm.OLS(y, X_const).fit()

# Get summary with p-values, confidence intervals, etc.
print(model.summary())

# Output includes:
# - RÂ², Adjusted RÂ²
# - F-statistic and its p-value
# - Coefficient p-values (is each predictor significant?)
# - Confidence intervals
```

### Sample Statsmodels Output

```
                            OLS Regression Results
==============================================================================
Dep. Variable:                  price   R-squared:                       0.850
Model:                            OLS   Adj. R-squared:                  0.845
                                        F-statistic:                     156.2
                                        Prob (F-statistic):           1.23e-45
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       12500.00    1200.00     10.417      0.000    10100.00   14900.00
size          150.50      12.30     12.236      0.000      126.20     174.80
bedrooms     5000.00    1500.00      3.333      0.001     2050.00    7950.00
==============================================================================
```

**Interpretation:**
- `P>|t|` < 0.05 means the predictor is **statistically significant**
- `size` coefficient 150.50: Each sq ft adds $150.50 to price
- `bedrooms` coefficient 5000: Each bedroom adds $5000 to price

---

## ðŸ”„ Regularization in Regression

### Why Regularize?

Prevent overfitting by **penalizing large coefficients**.

### Ridge Regression (L2)

```
Minimize: Î£(yáµ¢ - Å·áµ¢)Â² + Î» Î£Î²â±¼Â²

- Shrinks coefficients toward zero
- Keeps all features
- Good for multicollinearity
```

### Lasso Regression (L1)

```
Minimize: Î£(yáµ¢ - Å·áµ¢)Â² + Î» Î£|Î²â±¼|

- Can shrink coefficients to exactly zero
- Performs feature selection
- Good for sparse models
```

### Elastic Net

```
Minimize: Î£(yáµ¢ - Å·áµ¢)Â² + Î»â‚ Î£|Î²â±¼| + Î»â‚‚ Î£Î²â±¼Â²

- Combines L1 and L2
- Best of both worlds
```

### Comparison

| Method | Penalty | Feature Selection | Best For |
|--------|---------|-------------------|----------|
| **OLS** | None | No | Interpretability |
| **Ridge** | L2 (Î²Â²) | No | Multicollinearity |
| **Lasso** | L1 (\|Î²\|) | Yes | Feature selection |
| **Elastic Net** | L1 + L2 | Yes | Many correlated features |

---

## ðŸ’¡ Interview Tips

1. **Know the formulas** â€” RÂ², Adjusted RÂ², MSE, RMSE
2. **Explain assumptions** â€” linearity, independence, homoscedasticity, normality, no multicollinearity
3. **Understand RÂ² limitations** â€” doesn't indicate model correctness, can be misleading
4. **Discuss regularization** â€” when and why to use Ridge vs Lasso
5. **Know dummy variable trap** â€” why drop one category
6. **Connect to bias-variance** â€” simple model = high bias, complex = high variance

---

## â“ Common Interview Questions

| Question | Key Points |
|----------|------------|
| What is RÂ²? | Proportion of variance explained; 0 to 1 (usually) |
| Can RÂ² be negative? | Yes! If model is worse than predicting mean |
| RÂ² vs Adjusted RÂ²? | Adjusted penalizes for adding useless predictors |
| What are regression assumptions? | Linearity, independence, homoscedasticity, normality, no multicollinearity |
| How to handle categorical variables? | Dummy encoding, drop one category |
| When to use Ridge vs Lasso? | Ridge: multicollinearity. Lasso: feature selection |
| What is multicollinearity? | Predictors highly correlated, unstable coefficients |
| How to detect overfitting? | Compare train vs test performance, use cross-validation |
| What does the coefficient mean? | Change in y for one-unit change in x (holding others constant) |
| How to interpret p-values in regression? | Predictor significant if p < 0.05 |

---

## ðŸ”— Related Concepts

- **Bias-Variance Tradeoff** â€” Model complexity trade-off
- **Cross-Validation** â€” Model evaluation technique
- **Feature Engineering** â€” Creating meaningful predictors
- **Gradient Descent** â€” Optimization algorithm
- **Logistic Regression** â€” Classification (not regression!)

---

## ðŸ”¬ Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   REGRESSION SUMMARY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Simple Linear:    y = Î²â‚€ + Î²â‚x                             â”‚
â”‚  Multiple Linear:  y = Î²â‚€ + Î²â‚xâ‚ + Î²â‚‚xâ‚‚ + ... + Î²â‚™xâ‚™        â”‚
â”‚                                                             â”‚
â”‚  RÂ² = 1 - (SSR/SST) = 1 - Î£(y-Å·)Â² / Î£(y-È³)Â²                 â”‚
â”‚                                                             â”‚
â”‚  Adjusted RÂ² = 1 - [(1-RÂ²)(n-1)] / (n-k-1)                  â”‚
â”‚                                                             â”‚
â”‚  Key Metrics:                                               â”‚
â”‚    MSE  = (1/n) Î£(y-Å·)Â²                                     â”‚
â”‚    RMSE = âˆšMSE                                              â”‚
â”‚    MAE  = (1/n) Î£|y-Å·|                                      â”‚
â”‚                                                             â”‚
â”‚  Regularization:                                            â”‚
â”‚    Ridge (L2): Shrinks coefficients                         â”‚
â”‚    Lasso (L1): Feature selection                            â”‚
â”‚                                                             â”‚
â”‚  Assumptions: Linear, Independent, Homoscedastic,           â”‚
â”‚               Normal residuals, No multicollinearity        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

