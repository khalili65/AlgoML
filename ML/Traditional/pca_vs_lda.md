# PCA vs LDA: Dimensionality Reduction

> Two fundamental techniques for reducing dimensions, but with very different objectives.

---

## ğŸ“Š Overview

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Full Name** | Principal Component Analysis | Linear Discriminant Analysis |
| **Type** | Unsupervised | Supervised |
| **Goal** | Maximize variance | Maximize class separability |
| **Uses Labels?** | âŒ No | âœ… Yes |
| **Best For** | Signal representation, compression | Classification tasks |

---

## ğŸ¯ The Key Difference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PCA vs LDA VISUALIZATION                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Feature 2                                                  â”‚
â”‚     â†‘            2 2                                        â”‚
â”‚     â”‚          2 2 2 2                                      â”‚
â”‚     â”‚    1 1 1   2 2 2 2                                    â”‚
â”‚     â”‚  1 1 1 1 1   2 2 2                                    â”‚
â”‚     â”‚    1 1 1 1     2 2 2                                  â”‚
â”‚     â”‚      1 1 1 1     2 2                                  â”‚
â”‚     â”‚        1 1 1       2                                  â”‚
â”‚     â”‚          1 1 1                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Feature 1                  â”‚
â”‚                                                             â”‚
â”‚   PCA Direction: / (maximum variance - ignores classes)     â”‚
â”‚   LDA Direction: \ (maximum class separation)               â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PCA** finds directions of **maximum variance** in the data â€” it doesn't care about class labels.

**LDA** finds directions that **best separate classes** â€” it explicitly uses class labels.

---

## ğŸ”µ PCA: Principal Component Analysis

### What It Does

PCA finds new axes (principal components) that:
1. Are **orthogonal** to each other
2. Capture the **maximum variance** in the data
3. Are ordered by the amount of variance explained

### Mathematical Objective

```
Maximize: Var(Xw) = w^T Î£ w

Subject to: ||w|| = 1
```

Where `Î£` is the covariance matrix of the data.

### Algorithm Steps

1. **Standardize** the data (mean=0, std=1)
2. Compute the **covariance matrix**
3. Calculate **eigenvalues and eigenvectors**
4. Sort eigenvectors by eigenvalues (descending)
5. Select top `k` eigenvectors
6. **Transform** data to new subspace

### When to Use PCA

âœ… **Use PCA when:**
- You want to reduce dimensions for **any task** (not just classification)
- You don't have labels
- You want to **visualize** high-dimensional data
- You need **data compression**
- You want to remove **multicollinearity**

### Key Properties

| Property | Description |
|----------|-------------|
| **Unsupervised** | Doesn't need class labels |
| **Variance-based** | Preserves directions with most spread |
| **Orthogonal** | Components are uncorrelated |
| **Linear** | Finds linear combinations of features |
| **Max Components** | min(n_samples, n_features) |

---

## ğŸ”´ LDA: Linear Discriminant Analysis

### What It Does

LDA finds new axes that:
1. **Maximize** the distance between class means
2. **Minimize** the variance within each class
3. Optimize for **class separability**

### Mathematical Objective

```
                    w^T S_B w
Maximize: J(w) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    w^T S_W w
```

Where:
- `S_B` = **Between-class** scatter matrix
- `S_W` = **Within-class** scatter matrix

### Algorithm Steps

1. Compute the **mean** of each class
2. Compute **within-class scatter matrix** S_W
3. Compute **between-class scatter matrix** S_B
4. Solve generalized eigenvalue problem: `S_B w = Î» S_W w`
5. Select top `k` eigenvectors
6. **Transform** data to new subspace

### When to Use LDA

âœ… **Use LDA when:**
- You have a **classification** task
- You have **labeled data**
- You want features optimized for **class separation**
- You want dimensionality reduction that **improves classification**

### Key Properties

| Property | Description |
|----------|-------------|
| **Supervised** | Requires class labels |
| **Separability-based** | Maximizes class distinction |
| **Max Components** | min(n_classes - 1, n_features) |
| **Assumes** | Normal distribution, equal covariance |
| **Can Classify** | Can be used directly as a classifier |

---

## âš–ï¸ Side-by-Side Comparison

| Aspect | PCA | LDA |
|--------|-----|-----|
| **Supervision** | Unsupervised | Supervised |
| **Optimization Goal** | Max variance | Max class separability |
| **Uses Labels** | No | Yes |
| **Max Components** | min(n, d) | min(c-1, d) |
| **Assumptions** | None | Gaussian, equal covariance |
| **When Classes Overlap** | May mix classes | Tries to separate |
| **Output** | Principal components | Discriminant components |
| **Primary Use** | Compression, visualization | Classification preprocessing |

Where: `n` = samples, `d` = features, `c` = classes

---

## ğŸ”¬ Mathematical Details

### PCA: Covariance Matrix Eigen-decomposition

```
Covariance Matrix: Î£ = (1/n) X^T X

Eigen-decomposition: Î£v = Î»v

Principal Components: Columns of V (eigenvectors)
Explained Variance: Î» (eigenvalues)
```

### LDA: Scatter Matrices

**Within-class scatter:**
```
S_W = Î£_c Î£_{xâˆˆc} (x - Î¼_c)(x - Î¼_c)^T
```

**Between-class scatter:**
```
S_B = Î£_c n_c (Î¼_c - Î¼)(Î¼_c - Î¼)^T
```

**Generalized eigenvalue problem:**
```
S_B w = Î» S_W w   â†’   S_W^(-1) S_B w = Î»w
```

---

## ğŸ“ˆ Example: When PCA Fails but LDA Succeeds

```
Scenario: Two classes with different means but same direction of spread

Class 1: â—â—â—â—â—â—â—â—â—â—  (spread along diagonal)
Class 2:           â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹  (spread along same diagonal)

PCA Result: Projects onto diagonal â†’ Classes MIXED
LDA Result: Projects perpendicular â†’ Classes SEPARATED
```

This happens because:
- **PCA** maximizes variance (the diagonal has most variance)
- **LDA** maximizes separation (perpendicular direction separates classes)

---

## ğŸ› ï¸ Practical Considerations

### When to Choose PCA

```python
# Use PCA for:
from sklearn.decomposition import PCA

# 1. General dimensionality reduction
pca = PCA(n_components=50)
X_reduced = pca.fit_transform(X)

# 2. Visualization
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X)

# 3. Removing multicollinearity before regression
pca = PCA(n_components=0.95)  # Keep 95% variance
X_clean = pca.fit_transform(X)
```

### When to Choose LDA

```python
# Use LDA for:
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# 1. Preprocessing for classification
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X, y)  # Note: requires labels!

# 2. As a classifier itself
lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
predictions = lda.predict(X_test)
```

### Common Pipeline

```python
# Often used together!
# Step 1: PCA to reduce noise and speed up
pca = PCA(n_components=100)
X_pca = pca.fit_transform(X)

# Step 2: LDA for class-optimized reduction
lda = LinearDiscriminantAnalysis(n_components=9)  # for 10 classes
X_lda = lda.fit_transform(X_pca, y)

# Step 3: Classification
classifier.fit(X_lda, y)
```

---

## âš ï¸ Limitations

### PCA Limitations
- May discard dimensions important for classification
- Sensitive to feature scaling
- Only captures linear relationships
- Components may be hard to interpret

### LDA Limitations
- Requires labeled data
- Assumes Gaussian distribution
- Assumes equal class covariance
- Max `c-1` components (limiting for many classes)
- Sensitive to outliers

---

## ğŸ’¡ Interview Tips

1. **Know the fundamental difference** â€” variance vs. class separation
2. **Explain when each is appropriate** â€” unsupervised vs. supervised scenarios
3. **Understand the math** â€” at least conceptually (covariance, scatter matrices)
4. **Mention limitations** â€” LDA's Gaussian assumption, PCA's variance focus
5. **Real-world example** â€” face recognition often uses PCA then LDA

---

## â“ Common Interview Questions

| Question | Key Points |
|----------|------------|
| What's the difference between PCA and LDA? | PCA: variance, unsupervised. LDA: separation, supervised |
| When would PCA fail but LDA succeed? | When max variance direction doesn't separate classes |
| What's the max number of LDA components? | min(n_classes - 1, n_features) |
| Can LDA be used as a classifier? | Yes, it models class distributions |
| What are PCA's assumptions? | Just needs numeric data; LDA assumes Gaussian |

---

## ğŸ”— Related Concepts

- **Curse of Dimensionality** â€” Why we need dimensionality reduction
- **t-SNE / UMAP** â€” Non-linear dimensionality reduction
- **Feature Selection** â€” Choosing subset of original features
- **Kernel PCA** â€” Non-linear extension of PCA
- **QDA** â€” Quadratic Discriminant Analysis (relaxes equal covariance)

---

## ğŸ”¬ Quick Reference

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PCA vs LDA SUMMARY                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  PCA (Principal Component Analysis)                         â”‚
â”‚    â€¢ Unsupervised (no labels)                               â”‚
â”‚    â€¢ Maximizes VARIANCE                                     â”‚
â”‚    â€¢ Best for: compression, visualization, preprocessing    â”‚
â”‚    â€¢ Max components: min(n_samples, n_features)             â”‚
â”‚                                                             â”‚
â”‚  LDA (Linear Discriminant Analysis)                         â”‚
â”‚    â€¢ Supervised (needs labels)                              â”‚
â”‚    â€¢ Maximizes CLASS SEPARABILITY                           â”‚
â”‚    â€¢ Best for: classification preprocessing                 â”‚
â”‚    â€¢ Max components: min(n_classes - 1, n_features)         â”‚
â”‚                                                             â”‚
â”‚  Rule of Thumb:                                             â”‚
â”‚    No labels? â†’ PCA                                         â”‚
â”‚    Classification task? â†’ Consider LDA                      â”‚
â”‚    Often: PCA first (speed), then LDA (separation)          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

